From: Talat Batheesh <talatb@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/core/rdma_core.c

Change-Id: Icf172e7fc8d54246725f7c91420bbd9f857ae6a8
---
 drivers/infiniband/core/rdma_core.c | 127 ++++++++++++++++++++++++++++++++++--
 1 file changed, 123 insertions(+), 4 deletions(-)

diff --git a/drivers/infiniband/core/rdma_core.c b/drivers/infiniband/core/rdma_core.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/infiniband/core/rdma_core.c
+++ b/drivers/infiniband/core/rdma_core.c
@@ -79,7 +79,13 @@ static int uverbs_try_lock_object(struct ib_uobject *uobj,
 	 */
 	switch (mode) {
 	case UVERBS_LOOKUP_READ:
+#ifdef HAVE_ATOMIC_FETCH_ADD_UNLESS
 		return atomic_fetch_add_unless(&uobj->usecnt, 1, -1) == -1 ?
+#elif defined(HAVE___ATOMIC_ADD_UNLESS)
+		return __atomic_add_unless(&uobj->usecnt, 1, -1) == -1 ?
+#else
+		return atomic_add_unless(&uobj->usecnt, 1, -1) == -1 ?
+#endif
 			-EBUSY : 0;
 	case UVERBS_LOOKUP_WRITE:
 		/* lock is exclusive */
@@ -297,13 +303,50 @@ static struct ib_uobject *alloc_uobj(struct ib_uverbs_file *ufile,
 
 static int idr_add_uobj(struct ib_uobject *uobj)
 {
+	int ret;
        /*
         * We start with allocating an idr pointing to NULL. This represents an
         * object which isn't initialized yet. We'll replace it later on with
         * the real object once we commit.
         */
+#ifdef HAVE_XARRAY
 	return xa_alloc(&uobj->ufile->idr, &uobj->id, NULL, xa_limit_32b,
 			GFP_KERNEL);
+#elif defined(HAVE_IDR_ALLOC)
+	idr_preload(GFP_KERNEL);
+	spin_lock(&uobj->ufile->idr_lock);
+
+	/*
+	 *          * We start with allocating an idr pointing to NULL. This
+	 *          represents an
+	 *                   * object which isn't initialized yet. We'll replace
+	 *                   it later on with
+	 *                            * the real object once we commit.
+	 *                                     */
+	ret = idr_alloc(&uobj->ufile->idr, NULL, 0,
+			min_t(unsigned long, U32_MAX - 1, INT_MAX), GFP_NOWAIT);
+	if (ret >= 0)
+		uobj->id = ret;
+
+	spin_unlock(&uobj->ufile->idr_lock);
+	idr_preload_end();
+
+	return ret < 0 ? ret : 0;
+#else
+retry:
+	if (!idr_pre_get(&uobj->ufile->idr, GFP_KERNEL))
+		return -ENOMEM;
+
+	spin_lock(&uobj->ufile->idr_lock);
+	ret = idr_get_new(&uobj->ufile->idr, NULL, &uobj->id);
+	spin_unlock(&uobj->ufile->idr_lock);
+
+	if (ret == -EAGAIN)
+		goto retry;
+
+	return ret;
+
+#endif
 }
 
 /* Returns the ib_uobject or an error. The caller should check for IS_ERR. */
@@ -313,11 +356,13 @@ lookup_get_idr_uobject(const struct uverbs_api_object *obj,
 		       enum rdma_lookup_mode mode)
 {
 	struct ib_uobject *uobj;
+	unsigned long idrno = id;
 
 	if (id < 0 || id > ULONG_MAX)
 		return ERR_PTR(-EINVAL);
 
 	rcu_read_lock();
+#ifdef HAVE_XARRAY
 	/*
 	 * The idr_find is guaranteed to return a pointer to something that
 	 * isn't freed yet, or NULL, as the free after idr_remove goes through
@@ -329,6 +374,27 @@ lookup_get_idr_uobject(const struct uverbs_api_object *obj,
 		uobj = ERR_PTR(-ENOENT);
 	rcu_read_unlock();
 	return uobj;
+#else
+	/* object won't be released as we're protected in rcu */
+	uobj = idr_find(&ufile->idr, idrno);
+	if (!uobj) {
+		uobj = ERR_PTR(-ENOENT);
+		goto free;
+	}
+
+	/*
+	 * The idr_find is guaranteed to return a pointer to something that
+	 * isn't freed yet, or NULL, as the free after idr_remove goes through
+	 * kfree_rcu(). However the object may still have been released and
+	 * kfree() could be called at any time.
+	 */
+	if (!kref_get_unless_zero(&uobj->ref))
+		uobj = ERR_PTR(-ENOENT);
+
+free:
+	rcu_read_unlock();
+	return uobj;
+#endif	
 }
 
 static struct ib_uobject *
@@ -437,15 +503,25 @@ alloc_begin_idr_uobject(const struct uverbs_api_object *obj,
 	if (ret)
 		goto uobj_put;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&uobj->cg_obj, uobj->context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (ret)
 		goto remove;
 
+#endif
 	return uobj;
 
+#ifdef HAVE_CGROUP_RDMA_H
 remove:
+#ifdef HAVE_XARRAY
 	xa_erase(&ufile->idr, uobj->id);
+#else
+ 	spin_lock(&ufile->idr_lock);
+ 	idr_remove(&ufile->idr, uobj->id);
+ 	spin_unlock(&ufile->idr_lock);
+#endif
+#endif
 uobj_put:
 	uverbs_uobject_put(uobj);
 	return ERR_PTR(ret);
@@ -503,10 +579,17 @@ struct ib_uobject *rdma_alloc_begin_uobject(const struct uverbs_api_object *obj,
 
 static void alloc_abort_idr_uobject(struct ib_uobject *uobj)
 {
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, uobj->context->device,
 			   RDMACG_RESOURCE_HCA_OBJECT);
-
+#endif
+#ifdef HAVE_XARRAY
 	xa_erase(&uobj->ufile->idr, uobj->id);
+#else
+	spin_lock(&uobj->ufile->idr_lock);
+	idr_remove(&uobj->ufile->idr, uobj->id);
+	spin_unlock(&uobj->ufile->idr_lock);
+#endif
 }
 
 static int __must_check destroy_hw_idr_uobject(struct ib_uobject *uobj,
@@ -529,15 +612,22 @@ static int __must_check destroy_hw_idr_uobject(struct ib_uobject *uobj,
 	if (why == RDMA_REMOVE_ABORT)
 		return 0;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, uobj->context->device,
 			   RDMACG_RESOURCE_HCA_OBJECT);
-
+#endif
 	return 0;
 }
 
 static void remove_handle_idr_uobject(struct ib_uobject *uobj)
 {
+#ifdef HAVE_XARRAY
 	xa_erase(&uobj->ufile->idr, uobj->id);
+#else
+	spin_lock(&uobj->ufile->idr_lock);
+	idr_remove(&uobj->ufile->idr, uobj->id);
+	spin_unlock(&uobj->ufile->idr_lock);
+#endif
 	/* Matches the kref in alloc_commit_idr_uobject */
 	uverbs_uobject_put(uobj);
 }
@@ -568,6 +658,7 @@ static void remove_handle_fd_uobject(struct ib_uobject *uobj)
 static int alloc_commit_idr_uobject(struct ib_uobject *uobj)
 {
 	struct ib_uverbs_file *ufile = uobj->ufile;
+#ifdef HAVE_XARRAY
 	void *old;
 
 	/*
@@ -579,7 +670,18 @@ static int alloc_commit_idr_uobject(struct ib_uobject *uobj)
 	 */
 	old = xa_store(&ufile->idr, uobj->id, uobj, GFP_KERNEL);
 	WARN_ON(old != NULL);
-
+#else
+	spin_lock(&ufile->idr_lock);
+	/*
+	 * We already allocated this IDR with a NULL object, so
+	 * this shouldn't fail.
+	 *
+	 * NOTE: Once we set the IDR we loose ownership of our kref on uobj.
+	 * It will be put by remove_commit_idr_uobject()
+	 */
+	WARN_ON(idr_replace(&ufile->idr, uobj, uobj->id));
+	spin_unlock(&ufile->idr_lock);
+#endif
 	return 0;
 }
 
@@ -712,13 +814,18 @@ void rdma_lookup_put_uobject(struct ib_uobject *uobj,
 
 void setup_ufile_idr_uobject(struct ib_uverbs_file *ufile)
 {
+#ifdef HAVE_XARRAY
 	xa_init_flags(&ufile->idr, XA_FLAGS_ALLOC);
+#else
+	spin_lock_init(&ufile->idr_lock);
+	idr_init(&ufile->idr);
+#endif
 }
 
 void release_ufile_idr_uobject(struct ib_uverbs_file *ufile)
 {
 	struct ib_uobject *entry;
-	unsigned long id;
+	int id;
 
 	/*
 	 * At this point uverbs_cleanup_ufile() is guaranteed to have run, and
@@ -728,12 +835,22 @@ void release_ufile_idr_uobject(struct ib_uverbs_file *ufile)
 	 *
 	 * This is an optimized equivalent to remove_handle_idr_uobject
 	 */
+
+#ifdef HAVE_XARRAY
 	xa_for_each(&ufile->idr, id, entry) {
 		WARN_ON(entry->object);
 		uverbs_uobject_put(entry);
 	}
 
 	xa_destroy(&ufile->idr);
+#else
+	compat_idr_for_each_entry(&ufile->idr, entry, id) {
+		WARN_ON(entry->object);
+		uverbs_uobject_put(entry);
+	}
+
+	idr_destroy(&ufile->idr);
+#endif
 }
 
 const struct uverbs_obj_type_class uverbs_idr_class = {
@@ -811,8 +928,10 @@ static void ufile_destroy_ucontext(struct ib_uverbs_file *ufile,
 			ib_dev->ops.disassociate_ucontext(ucontext);
 	}
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&ucontext->cg_obj, ib_dev,
 			   RDMACG_RESOURCE_HCA_HANDLE);
+#endif
 
 	rdma_restrack_del(&ucontext->res);
 
