From: Talat Batheesh <talatb@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/core/mad.c

Change-Id: Ic63ed85e4f7e2a88d6b9dd8754f845bd5bec188a
---
 drivers/infiniband/core/mad.c | 206 ++++++++++++++++++++++++++++++++++++++++--
 1 file changed, 199 insertions(+), 7 deletions(-)

diff --git a/drivers/infiniband/core/mad.c b/drivers/infiniband/core/mad.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/infiniband/core/mad.c
+++ b/drivers/infiniband/core/mad.c
@@ -45,7 +45,11 @@
 #include <linux/module.h>
 #include <linux/security.h>
 #include <linux/sysfs.h>
+#ifdef HAVE_XARRAY
 #include <linux/xarray.h>
+#elif defined (HAVE_IDR_LOCK)
+#include <linux/idr.h>
+#endif
 #include <rdma/ib_cache.h>
 
 #include "mad_priv.h"
@@ -88,10 +92,21 @@ module_param_named(send_queue_size, mad_sendq_size, int, 0444);
 MODULE_PARM_DESC(send_queue_size, "Size of send queue in number of work requests");
 module_param_named(recv_queue_size, mad_recvq_size, int, 0444);
 MODULE_PARM_DESC(recv_queue_size, "Size of receive queue in number of work requests");
-
+#ifdef HAVE_XARRAY
 /* Client ID 0 is used for snoop-only clients */
 static DEFINE_XARRAY_ALLOC1(ib_mad_clients);
 static u32 ib_mad_client_next;
+#elif defined(HAVE_IDR_LOCK)
+/*
+ * The mlx4 driver uses the top byte to distinguish which virtual function
+ * generated the MAD, so we must avoid using it.
+ */
+#define AGENT_ID_LIMIT		(1 << 24)
+static DEFINE_IDR(ib_mad_clients);
+#else
+static atomic_t ib_mad_client_id = ATOMIC_INIT(0);
+#endif //HAVE_IDR_LOCK
+
 static struct list_head ib_mad_port_list;
 
 /*
@@ -150,11 +165,19 @@ static int send_sa_cc_mad(struct ib_mad_send_wr_private *mad_send_wr,
  * Timeout FIFO functions - implements FIFO with timeout mechanism
  */
 
+#ifdef HAVE_TIMER_SETUP
 static void activate_timeout_handler_task(struct timer_list *t)
+#else
+static void activate_timeout_handler_task(unsigned long data)
+#endif
 {
 	struct to_fifo *tf;
 
+#ifdef HAVE_TIMER_SETUP
 	tf = from_timer(tf, t, timer);
+#else
+	tf = (struct to_fifo *)data;
+#endif
 	del_timer(&tf->timer);
 	queue_work(tf->workq, &tf->work);
 }
@@ -262,8 +285,16 @@ static struct to_fifo *tf_create(void)
 		spin_lock_init(&tf->lists_lock);
 		INIT_LIST_HEAD(&tf->to_head);
 		INIT_LIST_HEAD(&tf->fifo_head);
+#ifdef HAVE_TIMER_SETUP
 		timer_setup(&tf->timer, activate_timeout_handler_task, 0);
+#else
+		init_timer(&tf->timer);
+#endif
 		INIT_WORK(&tf->work, timeout_handler_task);
+#ifndef HAVE_TIMER_SETUP
+		tf->timer.data = (unsigned long)tf;
+		tf->timer.function = activate_timeout_handler_task;
+#endif
 		tf->timer.expires = jiffies;
 		tf->stop_enqueue = 0;
 		tf->num_items = 0;
@@ -811,30 +842,46 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 	/* Validate parameters */
 	qpn = get_spl_qp_index(qp_type);
 	if (qpn == -1) {
+#ifdef RATELIMIT_STATE_INIT 		
 		dev_dbg_ratelimited(&device->dev, "%s: invalid QP Type %d\n",
 				    __func__, qp_type);
+#else
+		dev_notice(&device->dev,"%s: invalid QP Type %d\n",__func__, qp_type);
+#endif
 		goto error1;
 	}
 
 	if (rmpp_version && rmpp_version != IB_MGMT_RMPP_VERSION) {
+#ifdef RATELIMIT_STATE_INIT 		
 		dev_dbg_ratelimited(&device->dev,
 				    "%s: invalid RMPP Version %u\n",
 				    __func__, rmpp_version);
+#else
+		dev_notice(&device->dev,"%s: invalid RMPP Version%u\n",__func__, rmpp_version);
+#endif
 		goto error1;
 	}
 
 	/* Validate MAD registration request if supplied */
 	if (mad_reg_req) {
 		if (mad_reg_req->mgmt_class_version >= MAX_MGMT_VERSION) {
+#ifdef RATELIMIT_STATE_INIT 		
 			dev_dbg_ratelimited(&device->dev,
 					    "%s: invalid Class Version %u\n",
 					    __func__,
 					    mad_reg_req->mgmt_class_version);
+#else
+		dev_notice(&device->dev,"%s: invalid Class Version %u\n",__func__, mad_reg_req->mgmt_class_version);
+#endif
 			goto error1;
 		}
 		if (!recv_handler) {
+#ifdef RATELIMIT_STATE_INIT 		
 			dev_dbg_ratelimited(&device->dev,
 					    "%s: no recv_handler\n", __func__);
+#else
+		dev_notice(&device->dev,"%s:  no recv_handler\n",__func__);
+#endif
 			goto error1;
 		}
 		if (mad_reg_req->mgmt_class >= MAX_MGMT_CLASS) {
@@ -844,9 +891,13 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 			 */
 			if (mad_reg_req->mgmt_class !=
 			    IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {
+#ifdef RATELIMIT_STATE_INIT 		
 				dev_dbg_ratelimited(&device->dev,
 					"%s: Invalid Mgmt Class 0x%x\n",
 					__func__, mad_reg_req->mgmt_class);
+#else
+		dev_notice(&device->dev,"%s: Invalid Mgmt Class 0x%x\n",__func__, mad_reg_req->mgmt_class);
+#endif
 				goto error1;
 			}
 		} else if (mad_reg_req->mgmt_class == 0) {
@@ -854,9 +905,13 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 			 * Class 0 is reserved in IBA and is used for
 			 * aliasing of IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE
 			 */
+#ifdef RATELIMIT_STATE_INIT 		
 			dev_dbg_ratelimited(&device->dev,
 					    "%s: Invalid Mgmt Class 0\n",
 					    __func__);
+#else
+		dev_notice(&device->dev,"%s: Invalid Mgmt Class 0\n",__func__);
+#endif
 			goto error1;
 		} else if (is_vendor_class(mad_reg_req->mgmt_class)) {
 			/*
@@ -864,19 +919,27 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 			 * ensure supplied OUI is not zero
 			 */
 			if (!is_vendor_oui(mad_reg_req->oui)) {
+#ifdef RATELIMIT_STATE_INIT 		
 				dev_dbg_ratelimited(&device->dev,
 					"%s: No OUI specified for class 0x%x\n",
 					__func__,
 					mad_reg_req->mgmt_class);
+#else
+		dev_notice(&device->dev,"%s: No OUI specified for class 0x%x\n",__func__, mad_reg_req->mgmt_class);
+#endif
 				goto error1;
 			}
 		}
 		/* Make sure class supplied is consistent with RMPP */
 		if (!ib_is_mad_class_rmpp(mad_reg_req->mgmt_class)) {
 			if (rmpp_version) {
+#ifdef RATELIMIT_STATE_INIT 		
 				dev_dbg_ratelimited(&device->dev,
 					"%s: RMPP version for non-RMPP class 0x%x\n",
 					__func__, mad_reg_req->mgmt_class);
+#else
+		dev_notice(&device->dev,"%s: RMPP version for non-RMPP class 0x%x\n",__func__, mad_reg_req->mgmt_class);
+#endif
 				goto error1;
 			}
 		}
@@ -887,9 +950,13 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 					IB_MGMT_CLASS_SUBN_LID_ROUTED) &&
 			    (mad_reg_req->mgmt_class !=
 					IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {
+#ifdef RATELIMIT_STATE_INIT 		
 				dev_dbg_ratelimited(&device->dev,
 					"%s: Invalid SM QP type: class 0x%x\n",
 					__func__, mad_reg_req->mgmt_class);
+#else
+		dev_notice(&device->dev,"%s: Invalid SM QP type: class 0x%x\n",__func__, mad_reg_req->mgmt_class);
+#endif
 				goto error1;
 			}
 		} else {
@@ -897,9 +964,13 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 					IB_MGMT_CLASS_SUBN_LID_ROUTED) ||
 			    (mad_reg_req->mgmt_class ==
 					IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {
+#ifdef RATELIMIT_STATE_INIT 		
 				dev_dbg_ratelimited(&device->dev,
 					"%s: Invalid GS QP type: class 0x%x\n",
 					__func__, mad_reg_req->mgmt_class);
+#else
+		dev_notice(&device->dev,"%s: Invalid GS QP type: class 0x%x\n",__func__, mad_reg_req->mgmt_class);
+#endif
 				goto error1;
 			}
 		}
@@ -914,8 +985,12 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 	/* Validate device and port */
 	port_priv = ib_get_mad_port(device, port_num);
 	if (!port_priv) {
+#ifdef RATELIMIT_STATE_INIT 		
 		dev_dbg_ratelimited(&device->dev, "%s: Invalid port %d\n",
 				    __func__, port_num);
+#else
+		dev_notice(&device->dev,"%s: Invalid port %d\n",__func__, port_num);
+#endif
 		ret = ERR_PTR(-ENODEV);
 		goto error1;
 	}
@@ -924,8 +999,12 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 	 * will not have QP0.
 	 */
 	if (!port_priv->qp_info[qpn].qp) {
+#ifdef RATELIMIT_STATE_INIT 		
 		dev_dbg_ratelimited(&device->dev, "%s: QP %d not supported\n",
 				    __func__, qpn);
+#else
+		dev_notice(&device->dev,"%s: QP %d not supported\n",__func__, qpn);
+#endif
 		ret = ERR_PTR(-EPROTONOSUPPORT);
 		goto error1;
 	}
@@ -972,7 +1051,7 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 		ret = ERR_PTR(ret2);
 		goto error4;
 	}
-
+#ifdef HAVE_XARRAY
 	/*
 	 * The mlx4 driver uses the top byte to distinguish which virtual
 	 * function generated the MAD, so we must avoid using it.
@@ -984,12 +1063,32 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 		ret = ERR_PTR(ret2);
 		goto error5;
 	}
+#elif defined (HAVE_IDR_LOCK)
+	idr_preload(GFP_KERNEL);
+	idr_lock(&ib_mad_clients);
+	ret2 = idr_alloc_cyclic(&ib_mad_clients, mad_agent_priv, 0,
+			AGENT_ID_LIMIT, GFP_ATOMIC);
+	idr_unlock(&ib_mad_clients);
+	idr_preload_end();
+
+	if (ret2 < 0) {
+		ret = ERR_PTR(ret2);
+		goto error5;
+	}
+	mad_agent_priv->agent.hi_tid = ret2;
+#else 
+	spin_lock_irq(&port_priv->reg_lock);
+	mad_agent_priv->agent.hi_tid = atomic_inc_return(&ib_mad_client_id);
+#endif /* HAVE_XARRAY  */
 
 	/*
 	 * Make sure MAD registration (if supplied)
 	 * is non overlapping with any existing ones
 	 */
+
+#ifdef HAVE_IDR_LOCK
 	spin_lock_irq(&port_priv->reg_lock);
+#endif
 	if (mad_reg_req) {
 		mgmt_class = convert_mgmt_class(mad_reg_req->mgmt_class);
 		if (!is_vendor_class(mgmt_class)) {
@@ -1000,7 +1099,11 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 				if (method) {
 					if (method_in_use(&method,
 							   mad_reg_req))
+#ifdef HAVE_IDR_LOCK
 						goto error6;
+#else
+						goto error5;
+#endif
 				}
 			}
 			ret2 = add_nonoui_reg_req(mad_reg_req, mad_agent_priv,
@@ -1016,24 +1119,47 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 					if (is_vendor_method_in_use(
 							vendor_class,
 							mad_reg_req))
-						goto error6;
+#ifdef HAVE_IDR_LOCK
+       					goto error6;
+#else
+						goto error5;
+#endif
 				}
 			}
 			ret2 = add_oui_reg_req(mad_reg_req, mad_agent_priv);
 		}
 		if (ret2) {
 			ret = ERR_PTR(ret2);
-			goto error6;
+#ifdef HAVE_IDR_LOCK
+				goto error6;
+#else
+				goto error5;
+#endif
 		}
 	}
+#ifndef HAVE_IDR_LOCK
+	/* Add mad agent into port's agent list */
+	list_add_tail(&mad_agent_priv->agent_list, &port_priv->agent_list);
+#endif
 	spin_unlock_irq(&port_priv->reg_lock);
 
 	trace_ib_mad_create_agent(mad_agent_priv);
 	return &mad_agent_priv->agent;
+#ifndef HAVE_IDR_LOCK
+error5:
+	spin_unlock_irq(&port_priv->reg_lock);
+#else
 error6:
 	spin_unlock_irq(&port_priv->reg_lock);
+#ifdef HAVE_XARRAY
 	xa_erase(&ib_mad_clients, mad_agent_priv->agent.hi_tid);
+#else
+ 	idr_lock(&ib_mad_clients);
+ 	idr_remove(&ib_mad_clients, mad_agent_priv->agent.hi_tid);
+ 	idr_unlock(&ib_mad_clients);
+#endif /*HAVE_XARRAY*/
 error5:
+#endif /*HAVE_IDR_LOCK*/
 	ib_mad_agent_security_cleanup(&mad_agent_priv->agent);
 error4:
 	kfree(reg_req);
@@ -1196,9 +1322,19 @@ static void unregister_mad_agent(struct ib_mad_agent_private *mad_agent_priv)
 
 	spin_lock_irq(&port_priv->reg_lock);
 	remove_mad_reg_req(mad_agent_priv);
+#ifndef HAVE_IDR_LOCK
+	list_del(&mad_agent_priv->agent_list);
+	spin_unlock_irq(&port_priv->reg_lock);
+#else
 	spin_unlock_irq(&port_priv->reg_lock);
+#ifdef HAVE_XARRAY
 	xa_erase(&ib_mad_clients, mad_agent_priv->agent.hi_tid);
-
+#else
+ 	idr_lock(&ib_mad_clients);
+ 	idr_remove(&ib_mad_clients, mad_agent_priv->agent.hi_tid);
+ 	idr_unlock(&ib_mad_clients);
+#endif /*HAVE_XARRAY*/
+#endif /*HAVE_IDR_LOCK*/
 	flush_workqueue(port_priv->wq);
 	ib_cancel_rmpp_recvs(mad_agent_priv);
 
@@ -1208,7 +1344,11 @@ static void unregister_mad_agent(struct ib_mad_agent_private *mad_agent_priv)
 	ib_mad_agent_security_cleanup(&mad_agent_priv->agent);
 
 	kfree(mad_agent_priv->reg_req);
+#ifndef HAVE_IDR_LOCK
+	kfree(mad_agent_priv);
+#else
 	kfree_rcu(mad_agent_priv, rcu);
+#endif
 }
 
 static void unregister_mad_snoop(struct ib_mad_snoop_private *mad_snoop_priv)
@@ -2381,6 +2521,24 @@ find_mad_agent(struct ib_mad_port_private *port_priv,
 	struct ib_mad_agent_private *mad_agent = NULL;
 	unsigned long flags;
 
+#ifndef HAVE_IDR_LOCK
+	spin_lock_irqsave(&port_priv->reg_lock, flags);
+	if (ib_response_mad(mad_hdr)) {
+		u32 hi_tid;
+		struct ib_mad_agent_private *entry;
+
+		/*
+		 * Routing is based on high 32 bits of transaction ID
+		 * of MAD.
+		*/
+		hi_tid = be64_to_cpu(mad_hdr->tid) >> 32;
+		list_for_each_entry(entry, &port_priv->agent_list, agent_list) {
+			if (entry->agent.hi_tid == hi_tid) {
+				mad_agent = entry;
+				break;
+			}
+		}
+#else
 	if (ib_response_mad(mad_hdr)) {
 		u32 hi_tid;
 
@@ -2390,10 +2548,15 @@ find_mad_agent(struct ib_mad_port_private *port_priv,
 		 */
 		hi_tid = be64_to_cpu(mad_hdr->tid) >> 32;
 		rcu_read_lock();
+#ifdef HAVE_XARRAY
 		mad_agent = xa_load(&ib_mad_clients, hi_tid);
+#else
+		mad_agent = idr_find(&ib_mad_clients, hi_tid);
+#endif
 		if (mad_agent && !atomic_inc_not_zero(&mad_agent->refcount))
 			mad_agent = NULL;
 		rcu_read_unlock();
+#endif
 	} else {
 		struct ib_mad_mgmt_class_table *class;
 		struct ib_mad_mgmt_method_table *method;
@@ -2402,7 +2565,9 @@ find_mad_agent(struct ib_mad_port_private *port_priv,
 		const struct ib_vendor_mad *vendor_mad;
 		int index;
 
+#ifdef HAVE_IDR_LOCK
 		spin_lock_irqsave(&port_priv->reg_lock, flags);
+#endif
 		/*
 		 * Routing is based on version, class, and method
 		 * For "newer" vendor MADs, also based on OUI
@@ -2442,6 +2607,7 @@ find_mad_agent(struct ib_mad_port_private *port_priv,
 							  ~IB_MGMT_METHOD_RESP];
 			}
 		}
+#ifdef HAVE_IDR_LOCK
 		if (mad_agent)
 			atomic_inc(&mad_agent->refcount);
 out:
@@ -2455,7 +2621,21 @@ out:
 		deref_mad_agent(mad_agent);
 		mad_agent = NULL;
 	}
-
+#else
+	}
+	if (mad_agent) { 
+		if (mad_agent->agent.recv_handler)
+			atomic_inc(&mad_agent->refcount);
+		else {
+			dev_notice(&port_priv->device->dev,
+				   "No receive handler for client %p on port %d\n",
+			           &mad_agent->agent, port_priv->port_num);
+				mad_agent = NULL;
+		}
+	}
+out:
+	spin_unlock_irqrestore(&port_priv->reg_lock, flags);
+#endif
 	return mad_agent;
 }
 
@@ -3840,6 +4020,9 @@ static int ib_mad_port_open(struct ib_device *device,
 	port_priv->device = device;
 	port_priv->port_num = port_num;
 	spin_lock_init(&port_priv->reg_lock);
+#ifndef HAVE_IDR_LOCK
+	INIT_LIST_HEAD(&port_priv->agent_list);
+#endif
 	init_mad_qp(port_priv, &port_priv->qp_info[0]);
 	init_mad_qp(port_priv, &port_priv->qp_info[1]);
 
@@ -4139,7 +4322,11 @@ static ssize_t sa_cc_attr_show(struct kobject *kobj,
 	return sa->show(cc_obj, buf);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops sa_cc_sysfs_ops = {
+#else
+static struct sysfs_ops sa_cc_sysfs_ops = {
+#endif
 	.show = sa_cc_attr_show,
 	.store = sa_cc_attr_store,
 };
@@ -4225,7 +4412,12 @@ int ib_mad_init(void)
 	mad_sendq_size = max(mad_sendq_size, IB_MAD_QP_MIN_SIZE);
 
 	INIT_LIST_HEAD(&ib_mad_port_list);
-
+#ifndef HAVE_XARRAY
+#ifdef HAVE_IDR_LOCK
+	/* Client ID 0 is used for snoop-only clients */
+	idr_alloc(&ib_mad_clients, NULL, 0, 0, GFP_KERNEL);
+#endif /*HAVE_IDR_LOCK*/
+#endif /*HAVE_XARRAY*/
 	if (ib_register_client(&mad_client)) {
 		pr_err("Couldn't register ib_mad client\n");
 		return -EINVAL;
